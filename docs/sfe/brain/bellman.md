## 제1장 벨만 방정식의 전제와 설계 최적화의 계층적 정식화

> 본 문서는 **LB-IGD (Laplace–Beltrami Inverse Game Design)** 관련 이론 문서입니다. (구현 코드: 미포함)
>
> 관련 문서: 제2장 `docs/brain/lbo.md` (Bellman → HJB → 확산 → 라플라시안/라플라스–벨트라미, 그리고 설계공간 2차 차분 정규화)

> 상위호환 정리(오해 방지): “벨만 방정식 = 라플라스–벨트라미”가 아닙니다. 이 문서 묶음에서 라플라시안/라플라스–벨트라미(\(\Delta,\Delta_g\))는 **연속시간 HJB에서 상태가 확률미분방정식(SDE)을 따를 때 생성자(generator)에 추가되는 2차항**으로 나타납니다(`lbo.md` 2–4절). 특히 **노이즈가 없는 결정론적 경우(\(\sigma\equiv 0\))에는 그 2차항이 사라져** 1차(HJB)로 정확히 환원됩니다.

### 초록
강화학습에서 벨만 방정식과 동적계획법(DP)은 보통 **고정된** 마르코프 결정과정(MDP) 위에서 성립합니다.
하지만 이 프로젝트가 다루는 “게임 설계”는 맵, 유닛, 팩션 규칙처럼 **MDP 자체를 바꾸는 상위 레벨 문제**입니다.

이 장에서는 아래를 차근차근 정리합니다.

- (i) **플레이 문제(정책 최적화)**와 **설계 문제(MDP 생성)**를 표기부터 분리해 정식화합니다.
- (ii) 왜 벨만/DP를 설계 레벨에 그대로 가져오기 어려운지(마르코프성/보상 계산 비용/상태공간 폭발) 핵심 이유를 설명합니다.
- (iii) 설계 문제를 “값으로만 평가되는 확률적 목적함수”로 보고, 이후 장의 ES(제3장)·평가 프로토콜(제4장)·LBO 안정화(제2장)로 연결합니다.


# 1. 서론: “플레이 최적화”와 “설계 최적화”는 다른 문제이다
일반적인 강화학습은 주어진 게임(규칙, 맵, 유닛 스펙)이 고정된 상태에서 최적 정책을 학습해 **승리 확률을 최대화**합니다.
하지만 우리가 하려는 일은 “이기는 방법”을 찾는 것이 아니라, 규칙/맵/유닛 스펙 자체를 조정해 **어느 한 팩션도 지속적으로 우위를 점하지 못하는 설계 \(x\)** 를 찾는 것입니다.

이를 위해서는 “플레이 레벨의 MDP”와 “설계 레벨의 최적화”를 표기부터 분리해 쓰는 것이 필수입니다.

#### 1.1 본 장에서 정리하는 것(읽기 가이드)
이 장의 목표는 “DP를 설계에 그대로 쓰자/말자” 같은 결론을 내리는 것이 아니라, 이후 장에서 사용할 **문제 틀을 단단하게 고정**하는 것입니다. 구체적으로는 다음을 합니다.

- **문제의 층위 분리**: 플레이 레벨(고정 MDP)과 설계 레벨(MDP 생성)을 수학적으로 구분해 표기합니다.
- **적용 한계 정리**: 벨만/DP가 설계 레벨에 직접 적용되기 어려운 이유를 구조적 조건(전이, 마르코프성, 재귀 분해 가능성) 관점에서 설명합니다.
- **문제 분류 고정**: 설계 문제를 확률적 black-box 및 bilevel 최적화로 정식화해, 이후 장의 알고리즘(ES/LBO/평가)로 연결합니다.


### 2. 배경: MDP와 벨만 방정식

#### 2.1 MDP 정의

강화학습에서 환경은 다음의 5-튜플로 주어진다.

$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \gamma).
$$

- $\mathcal{S}$: 상태 공간
- $\mathcal{A}$: 행동 공간
- $P(s' \mid s, a)$: 전이 확률
- $r(s,a)$: 보상
- $\gamma \in [0,1)$: 할인율

#### 2.2 마르코프성과 벨만 기대 방정식

마르코프성은 다음 조건으로 표현된다.

$$
P(s_{t+1}\mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = P(s_{t+1}\mid s_t, a_t).
$$

정책 $\pi$ 하에서 가치함수는

$$
V^\pi(s) \;=\; \mathbb{E}_\pi \big[r(s_t,a_t) + \gamma V^\pi(s_{t+1}) \;\big|\; s_t=s \big]
$$

를 만족한다. 이 “재귀 구조”가 DP의 핵심 기반이다.

#### 2.3 최적 벨만 방정식
최적 가치함수는 다음을 만족한다.

$$
V^*(s) \;=\; \max_{a\in\mathcal{A}}\Big( r(s,a) + \gamma\,\mathbb{E}_{s'\sim P(\cdot\mid s,a)}[V^*(s')] \Big).
$$

이 식의 전제는 “상태 $s$가 시간에 따라 연속적으로 전이되는 동일한 문제” 안에서 하위 문제로 분해된다는 점이다(최적성 원리).


# 3. 설계 문제의 정식화: $x \mapsto \mathcal{M}_x \mapsto \pi_x^* \mapsto y(x)$

#### 3.1 설계 변수
게임 설계는 맵 구조, 성/거점 배치, 유닛 개수/사거리/이동력, 팩션 구성 등 환경 자체의 구조를 바꾸는 변수들의 집합이다. 이를 하나의 벡터로 모아

$$
x \in \mathcal{X}
$$

로 둔다(연속/이산 혼합 가능).

## 3.2 설계가 만드는 “플레이 레벨 MDP”
각 설계 $x$는 서로 다른 환경(서로 다른 MDP)을 만든다.

$$
\mathcal{M}_x = (\mathcal{S}_x, \mathcal{A}_x, P_x, r_x, \gamma).
$$

여기서 $\mathcal{S}_x,\mathcal{A}_x,P_x,r_x$는 모두 $x$에 의해 변할 수 있다. 즉, 설계는 “정책을 최적화할 대상”의 정의 자체를 바꾼다.

## 3.3 내부(플레이) 최적화: $\pi_x^*$
설계 $x$가 주어지면, 플레이 레벨에서는 강화학습(혹은 self-play)로 최적 정책을 계산한다.

$$
\pi_x^* \in \arg\max_{\pi}\;\mathbb{E}_{\pi,\mathcal{M}_x}\Big[\sum_{t=0}^\infty \gamma^t r_x(s_t,a_t)\Big].
$$

이 내부 문제에서는 벨만 방정식이 (조건이 만족되는 한) 정상적으로 사용될 수 있다.

## 3.4 외부(설계) 평가: 관측 통계 $y(x)$
설계의 “좋음/나쁨”은 self-play(및 정책 학습)의 결과로 측정되는 통계량으로 주어진다.

$$
y(x;\omega) = \mathrm{Stats}\big(\pi_x^*;\omega\big),
$$

여기서 $\omega$는 랜덤 시드, 초기화, 학습 노이즈, 매칭 샘플링 등 평가의 확률성을 나타낸다. 예를 들어 $F$개 팩션의 승률을 $W_f(x;\omega)$로 측정할 수 있다.

#### 3.5 외부 목적함수: “밸런스”의 수학적 표현
밸런스를 “승률의 평탄화”로 정의하면 다음과 같은 손실을 사용할 수 있다.

$$
L_{\text{win}}(x;\omega) = \sum_{f=1}^{F}\big(W_f(x;\omega)-\tfrac{1}{F}\big)^2.
$$

최종적으로 외부 설계 문제는 기대값 최소화로 정리된다.

$$
J(x) \;=\; \mathbb{E}_{\omega}\big[L_{\text{win}}(x;\omega)\big],\qquad
x^* \in \arg\min_{x\in\mathcal{X}} J(x).
$$

### 4. 왜 벨만/DP가 “설계 레벨”에는 적용되지 않는가

#### 4.1 DP가 필요로 하는 구조
DP가 유효하려면 다음 두 조건이 핵심적으로 필요하다.

- **시간적 전이**: 상태가 시간에 따라 자연스럽게 전이되며, 동일한 문제의 하위 문제로 분해될 것
- **고정된 문제 공간**: 하위 문제들이 동일한 상태/전이/보상 구조(혹은 그 제한)를 공유할 것

플레이 레벨($\mathcal{M}_x$ 고정)에서는 이 조건들이 성립하기 쉽다. 설계 레벨에서는 그렇지 않다.

#### 4.2 설계 문제를 MDP로 “형식화”할 때 남는 구조적 난점
형식적으로는 설계 변수 $x$를 상태, 수정 연산 $a$를 행동으로 두어 “설계 MDP”를 구성할 수 있다. 그러나 이 형식화는 DP가 제공하는 계산적 이점을 거의 제공하지 못한다. 이를 다음 명제들로 정리한다.

#### 명제 1 (설계 레벨 보상의 비국소성 및 내부 최적화 의존성)
설계 레벨에서 한 번의 상태 전이(예: $x \to x'$)에 대한 보상 $r_{\text{design}}(x,a)$를 정의하려면, 통상적으로 $x'$가 만드는 환경 $\mathcal{M}_{x'}$에서 정책 학습(self-play 포함)을 수행하여 관측 통계 $y(x';\omega)$를 얻어야 한다. 즉, 보상 평가 자체가 내부 최적화(또는 근사 최적화)를 포함하며, “즉시 보상”으로서 계산적으로 저렴한 객체가 아니다.

**증명 개요.** 설계 보상은 보통 $L(y(x';\omega))$ 형태이며, $y(x';\omega)$는 $\pi_{x'}^*$의 self-play 결과로 정의된다. $\pi_{x'}^*$는 $\mathcal{M}_{x'}$에서의 학습 절차를 요구하므로, 설계 보상 계산은 내부 학습의 결과에 종속된다. 따라서 설계 단계의 보상은 상태 전이만으로 즉시 주어지는 값이 아니라, “학습을 실행한 뒤에야 관측되는 값”이 된다.

## 명제 2 (외부 상태 $x$만으로는 마르코프성이 보장되지 않는다)

외부 상태를 $x$로만 둘 경우, 관측 통계 $y(x;\omega)$의 분포는 학습 절차의 초기화/수렴 정도/탐색 정책 등 “학습 상태”에 의해 달라질 수 있다. 

특히 warm-start, 메타 적응, 정책 공유 등 실무적 설정에서는 동일한 $x$에 대해 결과 분포가 과거 히스토리에 의존할 수 있다.

**증명 개요.** 관측은 $y=y(x;\omega)$로 표현되며, $\omega$가 독립 동일 분포(i.i.d.)로 주어진다는 보장이 없다. 예를 들어 이전 설계에서 학습된 정책을 초기값으로 재사용하면 $\omega$는 사실상 “이전 학습 상태”를 포함한다. 따라서 외부에서의 “상태= $x$” 가정은 충분 조건이 되지 못한다. 마르코프성을 회복하려면 외부 상태를 $x$뿐 아니라 학습 알고리즘의 내부 상태(파라미터, 옵티마이저 상태, 리플레이 버퍼 등)까지 포함하도록 확장해야 하는데, 이는 상태공간을 과도하게 확대하며 DP의 목적(분해에 의한 효율)을 약화시킨다.

## 명제 3 (할인 누적보상과 설계 목적의 불일치)

플레이 레벨의 목표는 시간 누적 보상에 의해 자연스럽게 표현되지만, 설계 레벨의 목표는 통상 “설계의 품질을 단회 평가하여 최소화/최대화”하는 정적 목적 $J(x)$로 주어진다. 

이때 할인 누적보상으로 설계 목적을 재작성하는 것은 본질적 이점을 제공하지 않는다.

**증명 개요.** 설계는 물리적 시간 흐름에 대응하는 자연스러운 단계 $t$가 존재하지 않으며, 평가의 핵심은 $x$의 함수값(기대 손실)이다. 따라서 DP가 활용하는 재귀적 분해 구조를 설계 레벨에 도입하더라도, 문제의 본질적 비용(내부 학습/평가)과 불확실성(노이즈)은 제거되지 않는다.

결론적으로, 벨만/DP는 설계 레벨에서의 핵심 난제(내부 학습을 포함하는 평가, 확률적 관측, 상태공간 자체의 변화)를 “모형화로 숨길” 수는 있으나 “해결로 치환”하지는 못한다.

#### 4.3 (추가) 이산 설계변수와 상태공간 폭발: DP의 계산적 이점이 사라지는 경우
실제 설계에서는 다음과 같은 이산 변수가 필연적으로 등장한다.

- 유닛 수(정수 제약)
- 이동 패턴 선택(예: 패턴 ID)
- 맵 크기/장애물 배치(정수/조합 구조)

이때 설계 공간 $\mathcal{X}$는 연속 벡터공간이 아니라 **거대한 혼합(이산+연속) 공간**이 된다. 설계를 “시간에 따른 상태 전이”로 보려면, 설계 조작 연산의 시퀀스가 상태를 정의하게 되는데, 이는 다음 문제를 만든다.

- **상태의 의미가 시간축과 무관**: 설계 단계 $t$는 물리적 시간과 무관하며, ‘하위 문제’로 자연스럽게 분해되지 않는다.
- **전이 모델의 학습/정의가 곤란**: “설계 변경 $\to$ 성능 변화”는 내부 학습(정책 최적화)을 거치므로, 전이확률/보상을 닫힌 형태로 두기 어렵다.
- **조합 폭발**: 이산 설계변수는 DP가 기대하는 “국소적 재귀 계산으로 전역 최적을 얻는” 경로를 사실상 차단한다.

따라서 실무적으로는, 설계 레벨을 DP로 재표현하기보다는 `evaluation.md`의 프로토콜을 고정한 뒤 $\hat J(x)$를 값으로만 평가하는 black-box로 두고, ES 같은 방법으로 탐색하는 편이 단순하고(=KISS) 재현 가능하다.

### 5. 설계 문제의 올바른 분류: 확률적 black-box 최적화 및 bilevel 최적화
설계 문제는 다음의 bilevel 구조로 표현된다.

$$
\min_{x\in\mathcal{X}}\; \mathbb{E}_\omega\Big[L\big(y(x;\omega)\big)\Big]
$$

subject to

$$
\pi_x^* \in \arg\max_{\pi}\;\mathbb{E}_{\pi,\mathcal{M}_x}\Big[\sum_{t=0}^\infty \gamma^t r_x(s_t,a_t)\Big],
\qquad
y(x;\omega)=\mathrm{Stats}(\pi_x^*;\omega).
$$

여기서 벨만 방정식은 **내부(플레이) 문제**에서만 사용되고, 외부(설계) 문제는 본질적으로 확률적 black-box 최적화로 남습니다.

### 6. 연결: 왜 “공간/거리”가 핵심 설계 변수가 되는가
외부 목적 \(J(x)\)를 직접 최적화하려면, \(x\)의 변화가 통계량 \(y(x)\)에 미치는 효과가 “연속적이며, 저차원이고, 전역적으로 해석 가능”할수록 유리합니다.
제3장(`blackbox.md`)에서는 이 연결고리를 교전 거리 분포 \(p_x(d)\)로 잡아, 비미분적·확률적 목적에서 ES(Score Function Estimator)가 왜 자연스럽게 등장하는지 단계별로 유도합니다. 제5장(`inverse.md`)에서는 더 나아가 “목표 메타”에서 출발해 팩션 파라미터 자체를 생성하는 inverse design 정식화로 확장합니다.

---

### 7. 해결(실행) 관점 정리: 내부는 벨만, 외부는 프로토콜 고정된 black-box 최적화
본 장의 결론은 “설계 레벨을 DP로 바꾸어도 내부 학습/평가 비용과 확률성이 사라지지 않는다”는 점이다. 따라서 실제 해결 절차는 다음처럼 층위를 유지한 채 구성하는 것이 가장 단순하다(KISS).

- **내부(플레이) 레벨**: 각 $x$에 대해 $\mathcal{M}_x$를 고정하고, 그 위에서 $\pi_x^*$를 학습한다. 이때는 벨만/DP가 정상적으로 작동하는 문제(고정 MDP)로 취급한다.
- **외부(설계) 레벨**: $\hat J(x)$를 self-play 기반으로 추정하고, $\hat J$를 “값으로만 평가 가능한 확률적 함수”로 보고 ES 등으로 최적화한다. 이때 **평가 프로토콜(학습 예산/시드/매칭/초기화)** 을 고정하지 않으면 $x$ 비교가 성립하지 않는다(`evaluation.md`).

외부 목적은 승률 기반 밸런스를 중심으로 두고, 퇴화 해 방지를 위한 제약 및 (선택적으로) 분포 기반 보조 지표를 포함한다. 구체적인 실행 루프(프로토콜·제약·분산 감소 포함)는 `blackbox.md` 13절에 정리하였다.