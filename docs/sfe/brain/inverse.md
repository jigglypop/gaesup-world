## 제5장 역설계(Inverse Game Design): 목표 메타 기반 팩션·맵 생성

> 본 문서는 **LB-IGD (Laplace–Beltrami Inverse Game Design)** 관련 이론 문서입니다. (구현 코드: 미포함)
>
> - 관련 문서: 제1장 `docs/brain/bellman.md`, 제2장 `docs/brain/lbo.md`, 제3장 `docs/brain/blackbox.md`, 제4장 `docs/brain/evaluation.md`
> - 참고: 이 장은 “목표 메타 → 규칙/팩션 생성”을 위한 정식화 중심이며, 현재 코드에는 일부 개념(생성기 \(g\))이 아직 구현되어 있지 않습니다.

### 초록
제1장은 “플레이(정책 최적화)”와 “설계(MDP 생성)”를 분리해, 설계 문제를 black-box/bilevel 관점으로 정리했습니다. 제3장은 그런 외부 목적함수를 ES로 다루는 방법을 유도했습니다.

이 장은 한 걸음 더 나아가, **원하는 플레이 패턴(목표 메타)** 를 먼저 정하고 그 목표를 만족하도록 **팩션 파라미터와 맵/규칙을 역으로 구성**하는 문제를 설명합니다.
핵심 아이디어는 다음과 같습니다.

- (i) 팩션의 “정체성”을 self-play에서 관측되는 통계/분포로 정의합니다.
- (ii) 디자이너의 의도를 저차원 잠재변수 \(z_f\)로 표현합니다.
- (iii) 생성기 \(g(z_f;\phi)\)와 설계 \(x\)를 함께 최적화해, 관측 분포가 목표 분포와 정합되게 만듭니다.

이 과정에서 “분포를 맞추는 문제”가 자연스럽게 등장하며, Wasserstein 거리 \(W_2\)가 유용한 거리 척도로 사용됩니다.


### 1. 서론
제1장은 벨만/DP의 적용 범위를 “고정된 MDP”라는 조건 아래에서 정리했고, 제3장은 확률적·비미분 목적함수에서 ES(Score Function Estimator)가 왜 자연스럽게 등장하는지 보여줬습니다.
이 장은 그 기반 위에서, 목표 메타를 만족하는 설계 \(x\) 및 팩션 파라미터 \(\{\theta_f\}\)를 **역으로 구성하는 방법**을 다룹니다.

---

### 2. 문제 정의: “목표 메타”에서 “규칙/맵/팩션”으로
기존 접근은 대개 다음의 순방향 흐름을 가진다.

$$
\text{(팩션 파라미터, 맵)} \;\Rightarrow\; \text{관측 메타}.
$$

본 장이 다루는 역문제는 다음과 같다.

$$
\boxed{
\text{목표 메타(플레이 패턴)}
\;\Rightarrow\;
\text{팩션 파라미터 및 맵}
}
$$

여기서 “메타”는 단일 승률 수치가 아니라, 교전 거리/교전 빈도/전장 밀도/게임 길이 등 게임 플레이에서 관측되는 **분포적 통계량**으로 이해한다.

---

### 3. 관측가능한 “팩션 정체성”의 정의
팩션의 정체성은 디자이너의 서술이 아니라, 자기대전(self-play)에서 드러나는 통계적 패턴으로 정의할 수 있다. 예를 들어 팩션 $f$의 정체성 벡터를

$$
\mathcal{I}_f=(\mu_{d,f},\; \sigma_{d,f}^2,\; \rho_f)
$$

로 둔다.

- $\mu_{d,f}$ : 교전 거리의 평균
- $\sigma_{d,f}^2$ : 교전 거리의 분산
- $\rho_f$ : 전장 밀도(단위 면적당 교전 관련 이벤트/유닛 밀도 등)

이 정의는 (i) 관측 가능하고, (ii) 분포 기반 목적함수로 자연스럽게 연결되며, (iii) 맵/유닛 규칙 변화가 만들어내는 효과를 설명하기에 충분한 최소 요약 통계로 작동한다.


### 4. 인간 입력의 형식화: 잠재 의도 $z_f$
사람은 보통 “기동력 좋은 소수 정예”, “다수 근접 압박”, “원거리 포격형” 같은 서술로 의도를 표현한다. 이를 수학적으로는 저차원 잠재벡터로 표현한다.

$$
z_f \in \mathbb{R}^k
$$

여기서 $z_f$는 “의미 공간(latent intent)”이며, 실제 구현에서는 텍스트 임베딩 혹은 제한된 옵션 기반의 의미 벡터로 구성될 수 있다. 본 장의 논의는 $z_f$가 이미 주어진다고 가정한다.


### 5. 의도에서 파라미터로: 생성기 $g$와 팩션 파라미터 $\theta_f$
팩션 파라미터(유닛 수, 사거리, 이동력 등)를 $\theta_f \in \mathbb{R}^m$로 두고,

$$
\theta_f = g(z_f;\phi)
$$

를 학습 가능한 생성기(파라미터 $\phi$)로 둔다. $g$는 선형, MLP, 혹은 제약을 포함한 매핑으로 설계할 수 있으나, 본질은 “서술적 의도 $\to$ 게임 파라미터”의 함수화이다.

이때 $\theta_f$가 만드는 정체성 $\mathcal{I}_f$는 어떤 전방향 함수 $h$로 나타낼 수 있다.

$$
\mathcal{I}_f \approx h(\theta_f;\,x,\omega)
$$

여기서 $x$는 맵/규칙을 포함한 설계 변수, $\omega$는 학습/매칭의 확률성을 의미한다.

#### 5.2 (구현 관점) “팩션 파라미터”의 최소 표현: 커스텀 유닛 슬롯 + 패턴 ID
역설계를 실제 코드로 연결하려면, $\theta_f$를 “문장”이 아니라 **구체적인 파라미터 키 집합**으로 고정해야 한다. 본 프로젝트의 기본 템플릿은 다음 형태를 갖는다.

- 팩션 인덱스 $f\in\{0,1,\ldots,F-1\}$
- 커스텀 유닛 슬롯 $i\in\{0,1,2,3,4\}$ 및 킹 1개

팩션별 설계 파라미터는 예를 들어 아래처럼 쓸 수 있다.

- 맵/전역: `width`, `height`, `max_steps`, `no_attack_limit`, `shaping_scale`
- 팩션별 유닛 수: `p{f}_unit{i}_units`
- 팩션별 이동 패턴(이산): `p{f}_unit{i}_pattern` $\in\{0,\ldots,P-1\}$
- 팩션별 스탯(연속): `p{f}_unit{i}_move`, `p{f}_unit{i}_range`, `p{f}_unit{i}_damage`, `p{f}_unit{i}_hp`
- 팩션별 킹 스탯(연속): `p{f}_king_move`, `p{f}_king_range`, `p{f}_king_damage`, `p{f}_king_hp` (킹 수는 1개 고정)

이 구조는 다음 장점이 있다.

- **다팩션 확장**: $f$만 늘리면 $F$팩션으로 자연스럽게 확장된다.
- **이산+연속 혼합**: 패턴 선택(이산)과 수치 스탯(연속)을 한 설계 벡터에 같이 담을 수 있다.
- **역설계의 해석 가능성**: “이 팩션은 어떤 이동 패턴을 쓰고, 어떤 거리대에서 싸우도록 설계되었는가”를 명시적으로 추적할 수 있다.

#### 5.1 식별성(identifiability)과 정규화의 필요성

일반적으로 서로 다른 $(x,\phi)$가 동일하거나 매우 유사한 관측 메타를 유도할 수 있으므로, 역설계는 본질적으로 비식별(ill-posed) 문제가 된다. 

즉, 목표 메타 $q^\star$를 만족하는 해가 존재하더라도 그 해는 유일하지 않을 수 있으며, 관측 통계의 선택(예: 교전 거리 분포만 사용) 또한 해의 비유일성을 강화할 수 있다.

따라서 역설계에서는 “목표를 만족하는 해의 집합” 중 어떤 해를 선택할지에 대한 추가 기준이 필요하며, 이는 목적함수의 정규화/제약 항 $R(x,\phi)$로 구현된다. 예를 들어 비용 제약, 구현 가능성 제약(파라미터 범위), 게임 길이 제약, 이벤트 발생률 하한, 스타일 다양성 유도 등의 항을 포함하여, 통계적으로는 유사하나 설계 관점에서 부적절한 해가 선택되는 것을 방지한다.


### 6. “역설계 가능성”의 국소 선형화 근거
완전한 전역 역함수의 존재를 가정하기는 어렵다. 대신, 원하는 정체성 근처에서의 **국소 역추정 가능성**을 사용한다.

목표 정체성 $\mathcal{I}_f^\star$ 주변에서 $h$를 1차 테일러 전개하면,

$$
\Delta \mathcal{I}_f
\approx
J_h(\theta_f)\,\Delta \theta_f,
\qquad
J_h(\theta_f)=\frac{\partial h}{\partial \theta_f}.
$$

만약 $J_h$가 랭크 조건을 만족한다면, 최소자승 의미에서

$$
\Delta \theta_f \approx J_h(\theta_f)^{\dagger}\,\Delta \mathcal{I}_f
$$

가 성립한다($J_h^\dagger$: Moore–Penrose 의사역행렬). 이는 “사거리 증가 $\to$ 평균 교전 거리 증가” 같은 단조 경향이 존재할 때, 정체성 공간에서의 목표를 파라미터 공간으로 되돌리는 근사적 절차가 가능함을 설명한다.

이 관점은 다음을 시사한다.

- 전역적으로는 다대일/비선형/불연속이더라도,
- 실무적으로는 목표 구간에서의 국소 선형성과 제약을 이용해 역설계를 수행할 수 있다.

---

### 7. 최종 구조: (의도, 맵) $\to$ 관측 분포 $\to$ 손실
앞선 장(`bellman.md`, `blackbox.md`)의 표기와 정합되도록, 맵/규칙 설계 변수를 $x$로 통일하자(여기에는 맵 생성기의 파라미터도 포함될 수 있다).

각 $x$에 대해 플레이 레벨 MDP는 $\mathcal{M}_x$로 주어지고, 학습된 정책 $\pi_x^*$로부터 관측 통계가 수집된다.

팩션 집합 $\{z_f\}_{f=1}^{F}$가 주어졌을 때 전체 시스템은 다음 합성으로 정리된다.

$$
\{z_f\}_{f=1}^{F}
\xrightarrow{\;\theta_f=g(z_f;\phi)\;}
\{\theta_f\}_{f=1}^{F}
\xrightarrow{\;\text{game instantiation}\;}
\mathcal{M}_x
\xrightarrow{\;\text{RL/self-play}\;}
y(x,\phi;\omega).
$$

### 8. 목적함수: 목표 메타의 분포 정합
목표 메타를 목표 분포(혹은 목표 통계)로 두고, 관측 분포와의 거리를 최소화한다.

예를 들어 교전 거리 분포를 사용하면, 설계 $x$와 생성기 $\phi$가 만드는 경험 분포를 $p_{x,\phi}(d;\omega)$로 두고 목표 분포를 $q^\star(d)$로 둘 수 있다.

$$
J(x,\phi)
=
\mathbb{E}_\omega\Big[
W_2^2\big(p_{x,\phi}(\cdot;\omega),\,q^\star\big)
\Big]
 + \lambda\,\mathbb{E}_\omega\big[L_{\text{win}}(x,\phi;\omega)\big]
 + \beta\,R(x,\phi).
$$

- $W_2$: Wasserstein-2 거리(분포 정합)
- $L_{\text{win}}$: 승률 편향(불균형)을 직접 제어하는 항
- $R$: 현실적 제약(파라미터 범위, 비용, 난이도, 다양성 등)을 반영한 정규화/제약 항

#### 8.1 퇴화 해(degenerate solution)와 제약 항의 역할
분포 정합 또는 승률 평탄화만을 최소화할 경우, 다음과 같은 퇴화 해가 최적해로 선택될 수 있다.

- 교전 자체가 거의 발생하지 않도록 설계하여 관측 분포가 의미를 잃는 경우
- 극단적인 게임 길이(매우 짧거나 매우 긴)를 통해 관측 통계의 분산이 인위적으로 감소하는 경우

따라서 $R(x,\phi)$에는 이벤트 발생률, 게임 길이 범위, 비용/난이도 제한, 다양성 유도 등 설계가 의도하는 “게임성”을 보존하는 항을 포함하는 것이 바람직하다.

목표가 “팩션별 스타일을 동시에 만족”하는 것이라면, 팩션별 목표 분포 $q_f^\star$를 두고 합으로 정리할 수도 있다.

$$
\mathbb{E}_\omega\Big[ \sum_{f=1}^{F} W_2^2(p_{x,\phi,f}(\cdot;\omega),\,q_f^\star)\Big].
$$


### 9. 분포기하 관점: 에너지 최소화로서의 설계
분포 공간은 유클리드 공간이 아니라 거리/측도 구조를 갖는 공간으로 이해하는 것이 자연스럽다. 특히 $W_2$는 “질량 이동 비용”을 기반으로 하므로, 공간적/거리적 현상을 직접 모델링하는 교전 거리 분포와 잘 결합된다.

이 관점에서 설계 문제는 다음처럼 요약된다.

$$
\min_{x,\phi}\; \mathcal{E}(p_{x,\phi})
$$

여기서 $\mathcal{E}$는 목표 분포(메타)와의 거리 및 제약 항으로 구성된 에너지 함수이며, “맵은 분포를 변환하는 연산자”로 해석된다.


### 10. 최적화 관점: 외부는 black-box, 내부는 RL
본 장의 구조에서도 벨만 방정식은 플레이 레벨의 내부 문제에서만 의미를 갖는다. 외부에서 $(x,\phi)\mapsto J(x,\phi)$는 self-play/학습을 포함한 확률적 평가로 주어지므로, ES/CMA-ES 같은 방법이 자연스럽다(`blackbox.md`의 유도 참조).


### 11. 관련 연구 포지셔닝(개념적)
본 문제는 다음 연구 축들과 연결되지만 동일하지 않다.

- **Procedural Content Generation(PCG)**: 콘텐츠 생성이 목표이나, 본 문제는 “메타/공정성 분포”라는 분포적 목표를 직접 최적화한다.
- **Inverse Reinforcement Learning(IRL)**: 보상 함수 역추정이 핵심이나, 본 문제는 “환경/규칙/분포”를 역추정한다는 점에서 대상이 다르다.
- **Bilevel optimization / Meta-learning**: 내부 학습 절차를 포함한 외부 최적화라는 구조가 동일하다.
- **Differentiable games**: 미분 가능 환경을 가정하면 외부 그라디언트가 가능하나, 본 장은 일반적인 비미분적 설계 공간(이산 구조 포함)을 대상으로 한다.

---

### 12. 실험 프로토콜(재현 가능한 평가 설계)
문서 수준에서의 최소 실험 절차를 다음과 같이 제안한다.

1) **목표 정의**: 목표 메타 $q^\star$ 또는 $\{q_f^\star\}$를 분포/통계로 정의한다(예: 교전 거리 분포의 평균/분산, 밀도 목표).
2) **후보 생성**: $(x,\phi)$를 샘플링/업데이트하여 후보 설계를 만든다(ES/CMA-ES 등).
3) **평가**: 각 후보에서 여러 시드로 self-play를 수행하여 $p_{x,\phi}(\cdot;\omega)$, 승률 $W_f$, 기타 통계를 수집한다.
4) **목적 계산**: $W_2$ 및 승률 손실을 계산해 $J(x,\phi)$를 추정한다.
5) **안정성 검증**: 시드 변화에 대한 분산, 정책 수렴 정도에 대한 민감도를 보고한다.
6) **어블레이션**: (i) 분포 항만, (ii) 승률 항만, (iii) 결합, (iv) 제약 항 제거 등으로 비교한다.

## 13. 역설계를 “실행 가능한 문제”로 만들기: 식별성 완화(게이지 고정)와 정규화 템플릿
본 장의 역설계는 비식별(ill-posed)일 수 있으므로, 목표 메타를 만족하는 해의 집합 중 “어떤 해를 선택할지”를 명시해야 한다. 이를 위해 다음 두 요소를 최소로 포함한다.

#### 13.1 게이지 고정(gauge fixing): 동일 메타 해들 사이의 불필요한 자유도 제거
동일하거나 매우 유사한 관측 분포를 만드는 $(x,\phi)$가 여럿 존재할 때, 다음과 같은 선택 기준을 추가하여 해를 정준화(canonical)한다.

- **변경 최소**: 기준 설계 $x_{\text{ref}}$가 있을 때 $\|x-x_{\text{ref}}\|$를 작게(또는 변경된 컴포넌트 수를 작게) 유지
- **비용 최소**: 구현/밸런스 패치 비용(예: 파라미터 변경 폭, 규칙 복잡도)을 최소화
- **단순도**: 불필요한 파라미터 상호작용을 줄이는 방향으로 선택(예: 특정 그룹 파라미터만 조정)

이는 수학적으로는 정규화 항 $R(x,\phi)$의 일부로 흡수된다.

#### 13.2 정규화/제약 $R(x,\phi)$의 권장 구성(최소)
역설계에서의 $R$은 “목표 분포를 맞추면서도 게임성을 보존”하는 역할을 한다. 다음은 최소 템플릿이다.

- 퇴화 해 방지: 교전 발생률 하한, 게임 길이 범위, 정지/무승부 비율 상한(가능한 경우)
- 실현 가능성: 파라미터 범위/정수 제약/규칙 일관성 제약
- 선택 기준(게이지 고정): 변경 최소/비용 최소/단순도 항

구체 항목과 임계값은 `evaluation.md`의 “제약/강건성 체크”와 함께 고정한다.

#### 13.3 최종 목적함수(본 장 표기 정합)
본 장의 목적은 목표 메타 분포 정합을 중심으로 두되, 승률 기반 밸런스와 제약을 함께 포함하는 형태가 가장 안정적이다(`blackbox.md` 13절의 실행 루프와 연결).

$$
J(x,\phi)
=
\mathbb{E}_\omega\Big[
W_2^2\big(p_{x,\phi}(\cdot;\omega),\,q^\star\big)
\Big]
 + \lambda\,\mathbb{E}_\omega\big[L_{\text{win}}(x,\phi;\omega)\big]
 + \beta\,R(x,\phi).
$$

여기서 $q^\star$는 목표 메타 분포이며, 다팩션 목표가 혼합으로 주어지는 경우 `blackbox.md`의 $\bar q$와 같이 “공정 목표 혼합분포”로 둘 수 있다.